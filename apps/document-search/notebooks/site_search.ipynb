{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e58cfc8",
   "metadata": {},
   "source": [
    "# Site search with Webflow and Shaped\n",
    "\n",
    "This notebook demonstrates how to prepare, store, and retrieve documents from a Shaped relevance engine. We will use a real-world example of the Shaped Webflow site as an example. We will cover the following steps: \n",
    "1. Setup: Install dependencies and set Shaped API key\n",
    "2. Ingestion: Chunking and inserting documents into a Shaped custom dataset\n",
    "3. Inference: Making a text query to the Shaped relevance engine and retrieving results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7289ab4",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "\n",
    "## 1.1 Create virtual environment\n",
    "**Prerequisite:** Before you get started, create a new Python virtual environment using Python 3.11 and activate it: \n",
    "```\n",
    "cd /path/to/working/directory\n",
    "python3.11 -m venv .venv\n",
    "source ./.venv/bin/activate\n",
    "```\n",
    "\n",
    "## 1.2 Install dependencies via pip\n",
    "\n",
    "Then we'll install the needed libraries and set our API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d0b783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU shaped webflow langchain-text-splitters lxml python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4071a6e",
   "metadata": {},
   "source": [
    "## 1.3 Restart kernel\n",
    "\n",
    "After installing the packages, you may need to restart your kernel. In VSCode or Cursor, you can do this via the command palette. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa765311",
   "metadata": {},
   "source": [
    "## 1.4 Initialize Shaped CLI with your API key\n",
    "\n",
    "We will need to get a [free Shaped API key with write permissions](https://docs.shaped.ai/docs/support/getting-an-api-key). We'll then attach this to the Shaped Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2110edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file in the notebooks directory\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys from environment variables, prompt if not found\n",
    "if (os.getenv(\"SHAPED_API_KEY\") is None):\n",
    "    SHAPED_API_KEY = getpass(\"Please enter your Shaped API key: \")\n",
    "    os.environ[\"SHAPED_API_KEY\"] = SHAPED_API_KEY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce6bc07",
   "metadata": {},
   "source": [
    "# 2. Ingestion\n",
    "\n",
    "Next, we'll declare a new table to store our documents and insert rows to it. We'll cover the following steps in this section: \n",
    "1. Create a table with the Shaped API\n",
    "2. Get documents with the Webflow API\n",
    "3. Run the documents through a chunker\n",
    "4. Upload the document chunks to our Shaped table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5845c733",
   "metadata": {},
   "source": [
    "## 2.1 Get blog posts from Webflow\n",
    "\n",
    "The following code is how we get the documents from our Webflow CMS. This is useful to understand how document ingestion is done in the real-world; feel free to skip this section if you don't want to get bogged down in the details. \n",
    "\n",
    "### Get metadata\n",
    "\n",
    "We start by loading our environment variables and fetching category/author metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81a7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import select\n",
    "from webflow.client import Webflow\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Webflow bug workaround - \n",
    "# Monkey patch CollectionItemFieldData to support extra config (necessary to\n",
    "# retrieve pydantic.Extra.allow reference from same pydantic reference used by\n",
    "# webflow)\n",
    "try:\n",
    "    import pydantic.v1 as pydantic\n",
    "except ImportError:\n",
    "    import pydantic\n",
    "from webflow import CollectionItemFieldData\n",
    "CollectionItemFieldData.Config.extra = pydantic.Extra.allow\n",
    "\n",
    "# Load Webflow API key and consts from .env file\n",
    "def get_env_or_prompt(env_var_name: str, prompt_message: str) -> str:\n",
    "    \"\"\"Get environment variable, or prompt user if not set.\"\"\"\n",
    "    value = os.getenv(env_var_name)\n",
    "    if value is None:\n",
    "        value = getpass(prompt_message)\n",
    "        os.environ[env_var_name] = value\n",
    "    return value\n",
    "\n",
    "WEBFLOW_API_KEY = get_env_or_prompt(\"WEBFLOW_API_KEY\", \"Please enter your Webflow API key: \")\n",
    "WEBFLOW_SITE_ID = get_env_or_prompt(\"WEBFLOW_SITE_ID\", \"Please enter your Webflow Site ID: \")\n",
    "BLOG_COLLECTION_ID = get_env_or_prompt(\"WEBFLOW_BLOG_COLLECTION_ID\", \"Enter the Webflow collection ID for blog posts: \")\n",
    "CATEGORY_COLLECTION_ID = get_env_or_prompt(\"WEBFLOW_CATEGORY_COLLECTION_ID\", \"Enter the Webflow collection ID for categories: \")\n",
    "AUTHORS_COLLECTION_ID = get_env_or_prompt(\"WEBFLOW_AUTHORS_COLLECTION_ID\", \"Enter the Webflow collection ID for authors: \")\n",
    "ROLES_COLLECTION_ID = get_env_or_prompt(\"WEBFLOW_ROLES_COLLECTION_ID\", \"Enter the Webflow collection ID for roles: \")\n",
    "\n",
    "# initialize the webflow client\n",
    "webflowClient = Webflow(access_token=WEBFLOW_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701f761",
   "metadata": {},
   "source": [
    "### Get blog posts from the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682769cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 items (total: 100)\n",
      "Fetched 100 items (total: 200)\n",
      "Fetched 80 items (total: 280)\n"
     ]
    }
   ],
   "source": [
    "# Webflow API is paginated; we need to get documents 100 entries at a time\n",
    "items = []\n",
    "limit = 100\n",
    "offset = 0\n",
    "\n",
    "while True:\n",
    "    items_page = webflowClient.collections.items.list_items(\n",
    "        collection_id=BLOG_COLLECTION_ID,\n",
    "        limit=limit,\n",
    "        offset=offset,\n",
    "    )\n",
    "    \n",
    "    if items_page.items:\n",
    "        items.extend(items_page.items)\n",
    "        print(f\"Fetched {len(items_page.items)} items (total: {len(items)})\")\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    if len(items_page.items) < limit:\n",
    "        break\n",
    "    offset += limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac9814",
   "metadata": {},
   "source": [
    "### Construct dataframe\n",
    "\n",
    "Replace IDs with actual names to preserve semantic meaning\n",
    "\n",
    "Also, output all columns as strings and ensure there are no nested dicts (flatten column structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beebe5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog posts have been successfully saved to posts.jsonl.\n"
     ]
    }
   ],
   "source": [
    "import uuid \n",
    "\n",
    "blog_posts_df = pd.DataFrame()\n",
    "\n",
    "# we get category, author name, and target role so that we can enrich these columns with semantic info (not IDs)\n",
    "categories = webflowClient.collections.items.list_items(\n",
    "    collection_id=CATEGORY_COLLECTION_ID\n",
    ").dict().get(\"items\")\n",
    "\n",
    "authors = webflowClient.collections.items.list_items(\n",
    "    collection_id=AUTHORS_COLLECTION_ID\n",
    ").dict().get(\"items\")\n",
    "\n",
    "roles = webflowClient.collections.items.list_items(\n",
    "    collection_id=ROLES_COLLECTION_ID\n",
    ").dict().get(\"items\")\n",
    "\n",
    "for item in items:\n",
    "    item_id = item.id\n",
    "    \n",
    "    # Create a row dictionary starting with the item id\n",
    "    row = {\"id\": item.id}\n",
    "    \n",
    "    # Destructure field_data into separate columns\n",
    "    if hasattr(item, \"field_data\") and item.field_data:\n",
    "        # Convert field_data to dict to get all fields\n",
    "        field_data_dict = item.field_data.dict() if hasattr(item.field_data, \"dict\") else {}\n",
    "        # Merge field_data fields into the row\n",
    "        row.update(field_data_dict)\n",
    "    \n",
    "    # Replace IDs in `roles`, `author`, categories with strings\n",
    "    if (row.get(\"roles\")) is not None:\n",
    "        roles_names = []\n",
    "        for role in row[\"roles\"]:\n",
    "            role_name = next((r['fieldData']['name'] for r in roles if r.get('id') == role))\n",
    "            roles_names.append(role_name)\n",
    "        row[\"roles\"] = roles_names\n",
    "\n",
    "    if (row.get(\"categories\")) is not None:\n",
    "        categories_names = []\n",
    "        for category in row[\"categories\"]:\n",
    "            category_name = next((r['fieldData']['name'] for r in categories if r.get('id') == category))\n",
    "            categories_names.append(category_name)\n",
    "        row[\"categories\"] = categories_names\n",
    "\n",
    "    if (row.get(\"author\")) is not None:\n",
    "        row[\"author\"] = next((a['fieldData']['name'] for a in authors if a.get('id') == row[\"author\"]))\n",
    "\n",
    "    # convert lists and dicts into strings\n",
    "    for key, value in row.items():\n",
    "        if isinstance(value, list):\n",
    "            row[key] = \" \".join(str(v) for v in value)\n",
    "        if isinstance(value, dict):\n",
    "            row[key] = json.dumps(value)\n",
    "\n",
    "    \n",
    "\n",
    "    now_str = datetime.now().isoformat()\n",
    "    row[\"created_at\"] = now_str\n",
    "    row[\"updated_at\"] = now_str\n",
    "    \n",
    "    # Append row to dataframe\n",
    "    blog_posts_df = pd.concat([blog_posts_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "blog_posts_df.columns = blog_posts_df.columns.str.replace('-', '_') # replace hyphens with underscores\n",
    "\n",
    "blog_posts_df.to_json(\"data/posts.jsonl\", orient=\"records\", lines=True)\n",
    "blog_posts_df.to_json(\"data/posts.json\")\n",
    "print(\"Blog posts have been successfully saved to posts.jsonl.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c3e3d",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Now that we have our table of posts, we should extract sections to searches more relevant. To do this, we use a `chunking strategy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4b963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'name', 'slug', 'release_date', 'post_summary', 'author',\n",
      "       'read_length_in_mins', 'categories', 'post_body', 'main_image', 'roles',\n",
      "       'featured', 'popular', 'created_at', 'updated_at'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p9/1p33b6wd57n39qdrj2lh_t5r0000gn/T/ipykernel_49523/2170910125.py:16: LangChainBetaWarning: The class `HTMLSemanticPreservingSplitter` is in beta. It is actively being worked on, so the API may change.\n",
      "  splitter = HTMLSemanticPreservingSplitter(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 11438 chunks from 280 posts\n",
      "Columns and their types in blog_posts_chunked_df:\n",
      "id                             object\n",
      "name                           object\n",
      "slug                           object\n",
      "release_date                   object\n",
      "author                         object\n",
      "read_length_in_mins           float64\n",
      "categories                     object\n",
      "main_image                     object\n",
      "roles                          object\n",
      "featured                         bool\n",
      "popular                          bool\n",
      "created_at             datetime64[ns]\n",
      "updated_at             datetime64[ns]\n",
      "content                        object\n",
      "post_id                        object\n",
      "chunk_metadata                 object\n",
      "url                            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLSemanticPreservingSplitter\n",
    "\n",
    "# chunking step\n",
    "blog_posts_chunked_df = pd.DataFrame()\n",
    "\n",
    "# import posts from JSONL file\n",
    "posts = pd.read_json('data/posts.jsonl', lines=True)\n",
    "print(posts.columns)\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\")\n",
    "]\n",
    "\n",
    "splitter = HTMLSemanticPreservingSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \"],\n",
    "    max_chunk_size=50,\n",
    "    elements_to_preserve=[\"ul\", \"ol\", \"code\"],\n",
    "    denylist_tags=[\"script\", \"style\", \"head\"],\n",
    ")\n",
    "# Process all posts and create chunked dataframe\n",
    "blog_posts_chunked_df = pd.DataFrame()\n",
    "\n",
    "for idx, post in posts.iterrows():\n",
    "    post_body = post['post_body']\n",
    "    post_summary = post.get('post_summary', '')\n",
    "    if pd.isna(post_body):\n",
    "        post_body = ''\n",
    "    if pd.isna(post_summary):\n",
    "        post_summary = ''\n",
    "    \n",
    "    post_to_chunk = f\"<p>{post_summary}</p>\\n\" + post_body\n",
    "    \n",
    "    # Split the post into chunks\n",
    "    documents = splitter.split_text(post_to_chunk)\n",
    "    \n",
    "    # For each chunk, create a row with all post columns except post-summary and post-body\n",
    "    for doc in documents:\n",
    "        # Create a row dictionary with all post columns except post-summary and post-body\n",
    "        row = post.drop(['post_summary', 'post_body']).to_dict()\n",
    "        \n",
    "        # Add the chunk content\n",
    "        row['content'] = doc.page_content\n",
    "        \n",
    "        # Optionally add chunk metadata if it exists\n",
    "        if doc.metadata:\n",
    "            row['chunk_metadata'] = json.dumps(doc.metadata)\n",
    "\n",
    "        row['post_id'] = row['id']\n",
    "        row['id'] = str(uuid.uuid4())\n",
    "        \n",
    "        # Append to dataframe\n",
    "        blog_posts_chunked_df = pd.concat([blog_posts_chunked_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "print(f\"Created {len(blog_posts_chunked_df)} chunks from {len(posts)} posts\")\n",
    "blog_posts_chunked_df['url'] = \"https://www.shaped.ai/blog/\" + blog_posts_chunked_df['slug']\n",
    "\n",
    "blog_posts_chunked_df.to_json('data/blog_post_chunked.jsonl', orient='records', lines=True)\n",
    "# blog_posts_chunked_df.to_json('data/blog_post_chunked.json') # optional - not recommended\n",
    "\n",
    "print(\"Columns and their types in blog_posts_chunked_df:\")\n",
    "print(blog_posts_chunked_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b531d3",
   "metadata": {},
   "source": [
    "# Upload to Shaped\n",
    "\n",
    "Now that we have our data in semantic chunks, we can upload this data to Shaped. We'll use the Shaped CLI for this.\n",
    "\n",
    "First we need to create a schema for our table, to tell Shaped the column names and types. \n",
    "\n",
    "```yaml\n",
    "name: shaped_blog_posts_chunked\n",
    "schema_type: CUSTOM\n",
    "unique_keys: [id]\n",
    "column_schema:\n",
    "    id: String\n",
    "    name: String\n",
    "    slug: String\n",
    "    main_image: String\n",
    "    roles: String\n",
    "    author: String\n",
    "    categories: String\n",
    "    read_length_in_mins: Int32\n",
    "    popular: Bool\n",
    "    featured: Bool\n",
    "    created_at: DateTime\n",
    "    updated_at: DateTime\n",
    "    content: String\n",
    "    post_id: String\n",
    "    chunk_metadata: String\n",
    "    url: String\n",
    "```\n",
    "\n",
    "```\n",
    "shaped create-dataset --file\n",
    "```\n",
    "\n",
    "> Note: If you get an error - Module Not Found, remember to activate your virtual environment (`source ./.venv/bin/activate`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe8602",
   "metadata": {},
   "source": [
    "## Upload dataset schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184551c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"name\": \"shaped_blog_chunked\",\n",
    "    \"schema_type\": \"CUSTOM\",\n",
    "    \"unique_keys\": [\"id\"],\n",
    "    \"column_schema\": {\n",
    "        \"id\": \"String\",\n",
    "        \"name\": \"String\",\n",
    "        \"slug\": \"String\",\n",
    "        \"main_image\": \"String\",\n",
    "        \"roles\": \"String\",\n",
    "        \"author\": \"String\",\n",
    "        \"categories\": \"String\",\n",
    "        \"read_length_in_mins\": \"Float\",\n",
    "        \"popular\": \"Bool\",\n",
    "        \"featured\": \"Bool\",\n",
    "        \"created_at\": \"DateTime\",\n",
    "        \"updated_at\": \"DateTime\",\n",
    "        \"content\": \"String\",\n",
    "        \"post_id\": \"String\",\n",
    "        \"chunk_metadata\": \"String\",\n",
    "        \"url\": \"String\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save schema to YAML file\n",
    "import yaml\n",
    "\n",
    "with open(\"data/blog_posts_chunked.schema.yaml\", \"w\") as f:\n",
    "    yaml.dump(schema, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "# !shaped create-dataset --file data/blog_posts_chunked.schema.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d2beb",
   "metadata": {},
   "source": [
    "# Engine config\n",
    "\n",
    "Now we're going to create a simple engine to support semantic search on our document chunks. \n",
    "\n",
    "We will declare our configuration as a YAML file and upload it with the CLI.\n",
    "\n",
    "The most basic engine configuration has a `data` field with the data to fetch and the columns to index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f654c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"version\": \"v2\",\n",
      "  \"name\": \"blog_posts__simple_semantic_search_2\",\n",
      "  \"data\": {\n",
      "    \"item_dataset\": {\n",
      "      \"name\": \"blog_post_chunked\"\n",
      "    },\n",
      "    \"index\": {\n",
      "      \"search\": {\n",
      "        \"item_fields\": [\n",
      "          \"name\",\n",
      "          \"content\",\n",
      "          \"author\",\n",
      "          \"categories\",\n",
      "          \"roles\"\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "model_url: https://api.shaped.ai/v1/models/blog_posts__simple_semantic_search_2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "engine_config = {\n",
    "    \"version\" : \"v2\",\n",
    "    \"name\" : \"blog_posts__simple_semantic_search_3\",\n",
    "    \"data\" : {\n",
    "        \"item_dataset\" : {\n",
    "            \"name\" : \"blog_post_chunked\"\n",
    "        },\n",
    "        \"index\" : {\n",
    "            \"search\" : {\n",
    "                \"item_fields\" : [\n",
    "                    \"name\",\n",
    "                    \"content\",\n",
    "                    \"author\",\n",
    "                    \"categories\",\n",
    "                    \"roles\",\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"data/blog_posts_chunked.engine.yaml\", \"w\") as f:\n",
    "    yaml.dump(engine_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "!shaped create-model --file data/blog_posts_chunked.engine.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
